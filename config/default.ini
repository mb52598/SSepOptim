[DEFAULT]
number_of_speakers = 8
sample_rate = 16000

[Main]
#model = Sepformer
#model = DPTNet
#model = ConvTasNet
#model = TestModel
model = LSTMTasNet
#dataset = Aishell1Mix
#dataset = LibriMix
dataset = LibriCSS
optimizations = LowRankFactorization
#optimizations = Pruning,Quantization
#logs_path = logs/
log_level = INFO

[Training]
#id = DefaultTrainingScript
device = cuda
#seed = 1234567891011
epochs = 50
finetune_epochs = 10
batch_size = 2
gradient_accumulation_steps = 1
clip_grad_norm = 5.0
shuffle = True
num_workers = 4
checkpoint_epoch_log = 10
loss = ssepoptim.loss.scale_invariant_signal_to_distortion_ratio_pit_loss
convert_loss_to_permutation_invariant = no
load_last_checkpoint = False
save_finetune_checkpoint = False
apply_performance_optimizers = False
test_only = False
test_metrics = ssepoptim.metric.signal_to_distortion_ratio_fbss_pit,ssepoptim.metric.scale_invariant_signal_to_noise_ratio_pit,ssepoptim.metric.signal_to_noise_ratio_pit
calculate_test_metrics_improvement = True
checkpoints_path = checkpoints/
#observers = ssepoptim.training.observers.cuda_memory_observer.CudaMemoryObserver(),ssepoptim.training.observers.section_time_observer.SectionTimeObserver(),ssepoptim.training.observers.module_stats_observer.ModuleStatsObserver()
#observers = ssepoptim.training.observers.cuda_record_memory_history_observer.CudaRecordMemoryHistoryObserver(dump.pickle),ssepoptim.training.observers.torch_profiler_observer.TorchProfilerObserver(dumping.html,0,1,3,1)
early_stop = ssepoptim.training.early_stop.UnchangingValidationEarlyStop(30)
distributed_find_unused_params = False

[Aishell1MixDataset]
path = datasets_dir/Aishell1Mix/
num_frames_per_datapoint = 5000

[LibriMixDataset]
path = datasets_dir/LibriMix/
num_frames_per_datapoint = 5000

[LibriCSSDataset]
path = datasets_dir/LibriCSS/
num_frames_per_datapoint = 5000

[PruningOptimization]
method = weight-change-unstructured
amount = 0.2
layers = torch.nn.Linear,torch.nn.Conv1d,torch.nn.Conv2d,torch.nn.MultiheadAttention
num_iters = 1
model_validation_deterioration_delta = 0.00001
dataset_percentage = 0.2

[QuantizationOptimization]
method = QAT
dtype = qint8
implementation = Eager
layers = torch.nn.Conv1d,torch.nn.Conv2d

[LowRankFactorizationOptimization]
method = Tucker-HOSVD
keep_percentage = 0.9
num_iters = 2
apply_only_once = True

[TestModel]

[Sepformer]
N_encoder_out = 256
out_channels = 256
kernel_size = 16
kernel_stride = 8
num_spks = ${number_of_speakers}

[DPTNet]
n_src = ${number_of_speakers}
n_heads = 4
ff_hid = 256
chunk_size = 100
hop_size = 50
n_repeats = 2
norm_type = ssepoptim.libs.asteroid.GlobLN
ff_activation = torch.nn.ReLU
encoder_activation = torch.nn.ReLU
mask_act = torch.nn.Sigmoid
bidirectional = True
dropout = 0
kernel_size = 32
n_filters = 64
stride = 16

[ConvTasNet]
n_src = ${number_of_speakers}
#out_chan: Optional[int]
n_blocks = 8
n_repeats = 3
bn_chan = 128
hid_chan = 512
skip_chan = 128
conv_kernel_size = 3
norm_type = ssepoptim.libs.asteroid.GlobLN
mask_act = torch.nn.ReLU
causal = False
n_filters = 512
kernel_size = 16
stride = 8
encoder_activation = torch.nn.Identity

[LSTMTasNet]
n_src = ${number_of_speakers}
#out_chan: Optional[int]
hid_size = 500
mask_act = torch.nn.Sigmoid
bidirectional = True
rnn_type = torch.nn.LSTM
n_layers = 4
dropout = 0.3
encoder_activation = torch.nn.Identity
n_filters = 512
kernel_size = 40
stride = 20
